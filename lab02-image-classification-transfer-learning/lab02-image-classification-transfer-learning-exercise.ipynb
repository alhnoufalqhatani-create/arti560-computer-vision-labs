{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430e7f7b",
   "metadata": {},
   "source": [
    "##### ARTI 560 - Computer Vision  \n",
    "## Image Classification using Transfer Learning - Exercise \n",
    "\n",
    "### Objective\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "1. Select another pretrained model (e.g., VGG16, MobileNetV2, or EfficientNet) and fine-tune it for CIFAR-10 classification.  \n",
    "You'll find the pretrained models in [Tensorflow Keras Applications Module](https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n",
    "\n",
    "2. Before training, inspect the architecture using model.summary() and observe:\n",
    "- Network depth\n",
    "- Number of parameters\n",
    "- Trainable vs Frozen layers\n",
    "\n",
    "3. Then compare its performance with ResNet and the custom CNN.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "- Which model achieved the highest accuracy?\n",
    "- Which model trained faster?\n",
    "- How might the architecture explain the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26d77e",
   "metadata": {},
   "source": [
    "### Load and Prepare CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5xro33d6sb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\Desktop\\Alhnouf-lab1\\cv_lab\\Lib\\site-packages\\keras\\src\\datasets\\cifar.py:18: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  d = cPickle.load(f, encoding=\"bytes\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "num_classes = 10\n",
    "IMG_SIZE = 96   # resized to fit pretrained models\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def preprocess(images, labels):\n",
    "    images = tf.image.resize(images, (IMG_SIZE, IMG_SIZE))\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = tf.keras.applications.mobilenet_v2.preprocess_input(images)\n",
    "    return images, labels\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "            .shuffle(10000).batch(BATCH_SIZE) \\\n",
    "            .map(preprocess).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
    "            .batch(BATCH_SIZE) \\\n",
    "            .map(preprocess).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0q6f9oeo4w4r",
   "metadata": {},
   "source": [
    "### Load Pretrained Model (Backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "r69gnxhrso",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "mobilenet_base = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "# Freeze all backbone layers initially\n",
    "mobilenet_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8qewu6jmpv3",
   "metadata": {},
   "source": [
    "### Build Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eehvjmrc54p",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "x = mobilenet_base(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "mobilenet_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iszd7dzatqj",
   "metadata": {},
   "source": [
    "### Inspect Architecture Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "znoabnftxr",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_96             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_96             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m12,810\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,270,794</span> (8.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,270,794\u001b[0m (8.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> (50.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,810\u001b[0m (50.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network depth: 5\n",
      "Total parameters: 2270794\n",
      "Trainable parameters: 12810\n",
      "Frozen parameters: 2257984\n"
     ]
    }
   ],
   "source": [
    "mobilenet_model.summary()\n",
    "\n",
    "print(\"Network depth:\", len(mobilenet_model.layers))\n",
    "print(\"Total parameters:\", mobilenet_model.count_params())\n",
    "print(\"Trainable parameters:\",\n",
    "      sum(tf.size(w).numpy() for w in mobilenet_model.trainable_weights))\n",
    "print(\"Frozen parameters:\",\n",
    "      sum(tf.size(w).numpy() for w in mobilenet_model.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "msxhxjtnrcn",
   "metadata": {},
   "source": [
    "### Train with Frozen Backbone (Transfer Learning Stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sdu2qdfpuqb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 107ms/step - accuracy: 0.7902 - loss: 0.6195 - val_accuracy: 0.8424 - val_loss: 0.4425\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 111ms/step - accuracy: 0.8458 - loss: 0.4495 - val_accuracy: 0.8519 - val_loss: 0.4241\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 111ms/step - accuracy: 0.8563 - loss: 0.4202 - val_accuracy: 0.8580 - val_loss: 0.4091\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 96ms/step - accuracy: 0.8599 - loss: 0.4071 - val_accuracy: 0.8589 - val_loss: 0.4033\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 95ms/step - accuracy: 0.8585 - loss: 0.4024 - val_accuracy: 0.8602 - val_loss: 0.4055\n"
     ]
    }
   ],
   "source": [
    "mobilenet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = mobilenet_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7o5aodxkg0o",
   "metadata": {},
   "source": [
    "### Fine-Tuning Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "jzrjq2snfki",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers in backbone: 30 / 154\n",
      "Epoch 1/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 132ms/step - accuracy: 0.8132 - loss: 0.6043 - val_accuracy: 0.8511 - val_loss: 0.4734\n",
      "Epoch 2/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 133ms/step - accuracy: 0.8605 - loss: 0.4274 - val_accuracy: 0.8732 - val_loss: 0.4083\n",
      "Epoch 3/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 134ms/step - accuracy: 0.8797 - loss: 0.3607 - val_accuracy: 0.8791 - val_loss: 0.3809\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze backbone\n",
    "mobilenet_base.trainable = True\n",
    "\n",
    "# Freeze early layers and fine-tune last layers only\n",
    "for layer in mobilenet_base.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"Trainable layers in backbone:\",\n",
    "      sum(l.trainable for l in mobilenet_base.layers),\n",
    "      \"/\", len(mobilenet_base.layers))\n",
    "\n",
    "mobilenet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_ft = mobilenet_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9tee6ck8jkl",
   "metadata": {},
   "source": [
    "### Which model achieved the highest accuracy?\n",
    "\n",
    "ResNet50V2 (fine-tuned) achieved the highest test accuracy at approximately 0.9162, outperforming both MobileNetV2 and the custom CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbxnpxct1j7",
   "metadata": {},
   "source": [
    "### Which model trained faster?\n",
    "\n",
    "ResNet trained faster than MobileNetV2. Despite MobileNetV2 being a lighter architecture, ResNet converged more quickly during training on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5wzzp7ldrpe",
   "metadata": {},
   "source": [
    "### How might the architecture explain the differences?\n",
    "\n",
    "ResNet uses residual (skip) connections that allow gradients to flow through deeper layers more effectively, enabling richer feature learning and higher accuracy. MobileNetV2 relies on depthwise separable convolutions, which reduce computational cost and model size but sacrifice some representational capacity compared to ResNet. The custom CNN, with its simpler and shallower architecture, lacks the depth and pretrained features of both transfer learning models, resulting in lower overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc2e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
